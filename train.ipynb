{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bitdglconda2e96401c5587439a8bf114176530353d",
   "display_name": "Python 3.7.9 64-bit ('dgl': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(180000)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb04c041bb0>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import dgl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits as BCELoss\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "import torch.nn as nn\n",
    "\n",
    "import networkx as nx\n",
    "import json\n",
    "import models\n",
    "device = \"cuda\"\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading data\n",
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "print('Loading data')\n",
    "path = '/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/novelgridworlds_no_nov_1_easy/normal_graphs_maps.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    graphs,maps = pickle.load(f)\n",
    "\n",
    "\n",
    "path = '/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/novelgridworlds_no_nov_1_easy/normal_nodeids.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    node_ids = pickle.load(f)\n",
    "\n",
    "from Dataset import *\n",
    "\n",
    "data = Dataset(graphs,maps)\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn = my_collate)\n",
    "\n",
    "\n",
    "print('Loading data')\n",
    "#path = '/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/novelgridworlds_axe_1_hard/novel_graphs_maps.pkl'\n",
    "path = '/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/validation/valid_graphs_maps.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    nov_graphs,nov_maps = pickle.load(f)\n",
    "\n",
    "\n",
    "#path = '/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/novelgridworlds_axe_1_hard/novel_nodeids.pkl'\n",
    "path = '/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/validation/valid_nodeids.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    nov_node_ids = pickle.load(f)\n",
    "\n",
    "from Dataset import *\n",
    "test_data = Dataset(nov_graphs,nov_maps, ids =  nov_node_ids)\n",
    "targets = test_data.make_targets_from_ids()\n",
    "test_data.y = targets\n",
    "valid_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False, collate_fn = my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "\n",
    "def train_discriminator(disc, gen, batch_size,device,batch, opt_d):\n",
    "\n",
    "    pair = []\n",
    "    opt_d.zero_grad()\n",
    "    \n",
    "    graphs1,maps1,graphs2,maps2, real_targets = batch\n",
    "\n",
    "    graphs1 = graphs1.to(device)\n",
    "    graphs2 = graphs2.to(device)\n",
    "\n",
    "    maps1 = maps1.to(device)\n",
    "    maps2 = maps2.to(device)\n",
    "    real_targets = torch.Tensor(real_targets).to(device)\n",
    "\n",
    "    \n",
    "\n",
    "    steps = [graphs1,maps1,graphs2,maps2]\n",
    "    real_preds, im1 = disc(steps)\n",
    "    pair.append(im1.detach().cpu())\n",
    "\n",
    "    real_loss = torch.nn.functional.binary_cross_entropy(real_preds, real_targets)\n",
    "    real_score = torch.mean(real_preds).item()\n",
    "    \n",
    "    \n",
    "    action = gen(steps)\n",
    "    \n",
    "    fake_steps = models.make_step_modifications(action, steps)\n",
    "    \n",
    "    graphs1,maps1,graphs2,maps2, fake_targets = fake_steps\n",
    "    graphs1 = graphs1.to(device)\n",
    "    graphs2 = graphs2.to(device)\n",
    "    fake_targets = torch.zeros(len(maps2)).to(device)\n",
    "\n",
    "    maps1 = maps1.to(device)\n",
    "    maps2 = maps2.to(device)\n",
    "\n",
    "    fake_steps = [graphs1,maps1,graphs2,maps2]\n",
    "    \n",
    "    \n",
    "    \n",
    "    fake_preds,im2 = disc(fake_steps)\n",
    "    pair.append(im2.detach().cpu())\n",
    "    fake_loss = torch.nn.functional.binary_cross_entropy(fake_preds, fake_targets)\n",
    "    fake_score = torch.mean(fake_preds).item()\n",
    "\n",
    "    \n",
    "    imgs.append(pair)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    total_loss.backward()\n",
    "\n",
    "    opt_d.step()\n",
    "    return total_loss.item(), real_score, fake_score\n",
    "\n",
    "\n",
    "\n",
    "def train_generator(gen, disc, batch_size, opt_g, batch, device):\n",
    "    \n",
    "    opt_g.zero_grad()\n",
    "\n",
    "    graphs1,maps1,graphs2,maps2, real_targets = batch\n",
    "    graphs1 = graphs1.to(device)\n",
    "    graphs2 = graphs2.to(device)\n",
    "\n",
    "    maps1 = maps1.to(device)\n",
    "    maps2 = maps2.to(device)\n",
    "    real_targets = torch.ones(len(maps2)).to(device)\n",
    "\n",
    "    steps = [graphs1,maps1,graphs2,maps2]\n",
    "\n",
    "    action = gen(steps)\n",
    "    \n",
    "\n",
    "    fake_steps = make_step_modifications(action, steps)\n",
    "    \n",
    "\n",
    "    graphs1,maps1,graphs2,maps2, fake_targets = fake_steps\n",
    "    graphs1 = graphs1.to(device)\n",
    "    graphs2 = graphs2.to(device)\n",
    "\n",
    "    maps1 = maps1.to(device)\n",
    "    maps2 = maps2.to(device)\n",
    "    fake_targets = torch.zeros(len(maps2)).to(device)#torch.Tensor(fake_targets).to(device)\n",
    "\n",
    "    fake_steps = [graphs1,maps1,graphs2,maps2]\n",
    "    \n",
    "    \n",
    "    preds,_ = disc(fake_steps)\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss = torch.nn.functional.binary_cross_entropy(preds, fake_targets)\n",
    "    \n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    opt_g.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Training function. Takes in Discriminator and Generator. Generator can be just a random sampler of action. \n",
    "\n",
    "def fit(disc, gen, train_loader,epochs, device,lr_g = 1e-4, lr_d = 1e-4, start_idx=1, run_on_valid = False, test_loader = None,batch_size = 32, model_name = \"novelty_gan\"):\n",
    "    model_name = model_name+\"_best\"\n",
    "    f1s = []\n",
    "    torch.cuda.empty_cache()\n",
    "    disc.cuda()\n",
    "    gen.cuda()\n",
    "    disc.train()\n",
    "    gen.train()\n",
    "    # Losses & scores\n",
    "    losses_g = []\n",
    "    losses_d = []\n",
    "    real_scores = []\n",
    "    fake_scores = []\n",
    "    best_val_f1 = -np.inf\n",
    "    # Create optimizers\n",
    "    opt_d = torch.optim.Adam(disc.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "    opt_g = None\n",
    "    if gen is not None and type(gen).__name__ != \"RandomGenerator\":\n",
    "        opt_g = torch.optim.Adam(gen.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss_g = 0\n",
    "        loss_d = 0\n",
    "        counter = 0\n",
    "        real_score = 0\n",
    "        fake_score = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "            counter+=1\n",
    "            \n",
    "            # Train discriminator\n",
    "            loss_d, real_score, fake_score = train_discriminator(disc,gen,batch_size,device,batch, opt_d)\n",
    "            # Train generator\n",
    "            if opt_g is not None:\n",
    "                for j in range(1):#if counter % 1 == 0:#for j in range(5):\n",
    "                    \n",
    "                    loss_g = train_generator(gen,disc,batch_size,opt_g,batch, device)\n",
    "            else:\n",
    "                loss_g = 0\n",
    "                \n",
    "                \n",
    "        losses_g.append(loss_g)\n",
    "        losses_d.append(loss_d)\n",
    "        real_scores.append(real_score)\n",
    "        fake_scores.append(fake_score)\n",
    "        \n",
    "        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
    "            epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n",
    "        \n",
    "    \n",
    "        if run_on_valid:\n",
    "            precision, recall,f1,score = models.validate_model(disc,test_loader)\n",
    "            if f1>best_val_f1:\n",
    "                  best_val_f1 = f1\n",
    "                  torch.save(gen.state_dict(),os.path.join(\"./models\",model_name+\"_generator.model\"))\n",
    "                  torch.save(disc.state_dict(),os.path.join(\"./models\",model_name+\"_discriminator.model\"))\n",
    "                  file = open(os.path.join(\"./models\",model_name+\"_stats.json\"),'w')\n",
    "                  json.dump({\"precision\":precision,\"recall\":recall,\"f1\":f1},file)\n",
    "                  file.close()\n",
    "            f1s.append(f1)\n",
    "        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}, valid_precision: {:.4f}, valid_recall: {:.4f}, valid_F1: {:.4f}, valid_acc: {:.4f}\".format(\n",
    "            epoch+1, epochs, loss_g, loss_d, real_score, fake_score, precision,recall,f1, score))\n",
    "    \n",
    "    return losses_g, losses_d, real_scores, fake_scores, f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/45 [00:00<?, ?it/s]/home/panagiotis/.local/lib/python3.7/site-packages/torch/nn/functional.py:1614: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/panagiotis/.local/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/panagiotis/.conda/envs/dgl/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "/media/panagiotis/Samsung_T51/work/Research/Novelty_Detection/MapGraphs/models.py:354: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  actions = torch.nonzero(torch.round(action[k]))\n",
      "/media/panagiotis/Samsung_T51/work/Research/Novelty_Detection/MapGraphs/models.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  us = torch.tensor(us,dtype = torch.int32).to(device)\n",
      "/media/panagiotis/Samsung_T51/work/Research/Novelty_Detection/MapGraphs/models.py:244: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  norms = torch.tensor(norms, dtype= torch.float32).to(device)\n",
      "/media/panagiotis/Samsung_T51/work/Research/Novelty_Detection/MapGraphs/models.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  types = torch.tensor(torch.ones(len(vs)).to(device) + 3,dtype = torch.int32).to(device)\n",
      "/home/panagiotis/.conda/envs/dgl/lib/python3.7/site-packages/ipykernel_launcher.py:45: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      " 98%|█████████▊| 44/45 [01:01<00:01,  1.39s/it]/home/panagiotis/.conda/envs/dgl/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "/home/panagiotis/.conda/envs/dgl/lib/python3.7/site-packages/ipykernel_launcher.py:45: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "100%|██████████| 45/45 [01:01<00:00,  1.38s/it]\n",
      "Epoch [1/10], loss_g: 0.0000, loss_d: 0.5203, real_score: 0.9291, fake_score: 0.2067\n",
      "  0%|          | 0/45 [00:00<?, ?it/s]Epoch [1/10], loss_g: 0.0000, loss_d: 0.5203, real_score: 0.9291, fake_score: 0.2067, valid_precision: 0.1903, valid_recall: 0.9673, valid_F1: 0.3180, valid_acc: 0.4185\n",
      "100%|██████████| 45/45 [01:01<00:00,  1.37s/it]\n",
      "Epoch [2/10], loss_g: 0.0000, loss_d: 0.0281, real_score: 0.9758, fake_score: 0.0035\n",
      "  0%|          | 0/45 [00:00<?, ?it/s]Epoch [2/10], loss_g: 0.0000, loss_d: 0.0281, real_score: 0.9758, fake_score: 0.0035, valid_precision: 0.1879, valid_recall: 0.9709, valid_F1: 0.3149, valid_acc: 0.4077\n",
      "100%|██████████| 45/45 [01:00<00:00,  1.35s/it]\n",
      "Epoch [3/10], loss_g: 0.0000, loss_d: 0.0187, real_score: 0.9826, fake_score: 0.0010\n",
      "  0%|          | 0/45 [00:00<?, ?it/s]Epoch [3/10], loss_g: 0.0000, loss_d: 0.0187, real_score: 0.9826, fake_score: 0.0010, valid_precision: 0.1888, valid_recall: 0.9709, valid_F1: 0.3162, valid_acc: 0.4113\n",
      "100%|██████████| 45/45 [00:58<00:00,  1.30s/it]\n",
      "Epoch [4/10], loss_g: 0.0000, loss_d: 0.2553, real_score: 0.9851, fake_score: 0.1542\n",
      "  0%|          | 0/45 [00:00<?, ?it/s]Epoch [4/10], loss_g: 0.0000, loss_d: 0.2553, real_score: 0.9851, fake_score: 0.1542, valid_precision: 0.1724, valid_recall: 0.8400, valid_F1: 0.2861, valid_acc: 0.4123\n",
      "100%|██████████| 45/45 [00:54<00:00,  1.22s/it]\n",
      "Epoch [5/10], loss_g: 0.0000, loss_d: 0.0336, real_score: 0.9682, fake_score: 0.0005\n",
      "  0%|          | 0/45 [00:00<?, ?it/s]Epoch [5/10], loss_g: 0.0000, loss_d: 0.0336, real_score: 0.9682, fake_score: 0.0005, valid_precision: 0.1721, valid_recall: 0.8400, valid_F1: 0.2857, valid_acc: 0.4113\n",
      "100%|██████████| 45/45 [00:53<00:00,  1.20s/it]\n",
      "Epoch [6/10], loss_g: 0.0000, loss_d: 0.3447, real_score: 0.9812, fake_score: 0.1160\n",
      "  0%|          | 0/45 [00:00<?, ?it/s]Epoch [6/10], loss_g: 0.0000, loss_d: 0.3447, real_score: 0.9812, fake_score: 0.1160, valid_precision: 0.1895, valid_recall: 0.9200, valid_F1: 0.3143, valid_acc: 0.4373\n",
      "100%|██████████| 45/45 [01:04<00:00,  1.43s/it]\n",
      "Epoch [7/10], loss_g: 0.0000, loss_d: 0.0051, real_score: 0.9951, fake_score: 0.0002\n",
      "  0%|          | 0/45 [00:00<?, ?it/s]Epoch [7/10], loss_g: 0.0000, loss_d: 0.0051, real_score: 0.9951, fake_score: 0.0002, valid_precision: 0.1786, valid_recall: 0.8145, valid_F1: 0.2930, valid_acc: 0.4490\n",
      "100%|██████████| 45/45 [01:05<00:00,  1.45s/it]\n",
      "Epoch [8/10], loss_g: 0.0000, loss_d: 0.1227, real_score: 0.9173, fake_score: 0.0001\n",
      "  0%|          | 0/45 [00:00<?, ?it/s]Epoch [8/10], loss_g: 0.0000, loss_d: 0.1227, real_score: 0.9173, fake_score: 0.0001, valid_precision: 0.1870, valid_recall: 0.9491, valid_F1: 0.3124, valid_acc: 0.4144\n",
      "100%|██████████| 45/45 [01:02<00:00,  1.40s/it]\n",
      "Epoch [9/10], loss_g: 0.0000, loss_d: 0.0126, real_score: 0.9890, fake_score: 0.0014\n",
      "  0%|          | 0/45 [00:00<?, ?it/s]Epoch [9/10], loss_g: 0.0000, loss_d: 0.0126, real_score: 0.9890, fake_score: 0.0014, valid_precision: 0.1674, valid_recall: 0.8036, valid_F1: 0.2771, valid_acc: 0.4123\n",
      "100%|██████████| 45/45 [01:03<00:00,  1.40s/it]\n",
      "Epoch [10/10], loss_g: 0.0000, loss_d: 0.0054, real_score: 0.9957, fake_score: 0.0011\n",
      "Epoch [10/10], loss_g: 0.0000, loss_d: 0.0054, real_score: 0.9957, fake_score: 0.0011, valid_precision: 0.1910, valid_recall: 0.9855, valid_F1: 0.3200, valid_acc: 0.4128\n",
      "CPU times: user 49min 32s, sys: 38.4 s, total: 50min 11s\n",
      "Wall time: 15min 21s\n"
     ]
    }
   ],
   "source": [
    "#Initialize models and Fit. \n",
    "model_predictor = 'agent'\n",
    "disc = models.MapGraphModel(50,16, model_predictor=model_predictor)\n",
    "device = \"cuda\"\n",
    "disc.to(device)\n",
    "gen =models.RandomGenerator(batch_size)#MapGraphNoveltyInjector(50,16)\n",
    "gen.to(device)\n",
    "\n",
    "\n",
    "#opt_g = torch.optim.Adam(gen.parameters(), lr = 1e-3, betas=(0.5, 0.999))\n",
    "epochs = 10\n",
    "\n",
    "%time f1s_replay = fit(disc, gen, train_loader,epochs, device = device,lr_d = 1e-3, lr_g = 1e-3, test_loader=valid_loader, run_on_valid= True,batch_size = batch_size, model_name=f\"novelty_gan_{model_predictor}_test\")[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MapGraphModel(\n",
       "  (agent_layer): Sequential(\n",
       "    (0): Linear(in_features=160, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (GCN1): GraphConv(in=50, out=50, normalization=both, activation=None)\n",
       "  (GCN2): GraphConv(in=50, out=50, normalization=both, activation=None)\n",
       "  (GCN3): GraphConv(in=50, out=16, normalization=both, activation=None)\n",
       "  (Conv): Sequential(\n",
       "    (0): Conv2d(16, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): Tanh()\n",
       "  )\n",
       "  (DiffConv): Sequential(\n",
       "    (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Conv2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Conv2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "disc = models.MapGraphModel(50,16)\n",
    "device = \"cuda\"\n",
    "disc.to(device)\n",
    "gen = models.RandomGenerator(batch_size)#MapGraphNoveltyInjector(50,8)\n",
    "gen.to(device)\n",
    "gen.load_state_dict(torch.load(f\"./models/novelty_gan_{model_predictor}_test_best_generator.model\"))\n",
    "disc.load_state_dict(torch.load(f\"./models/novelty_gan_{model_predictor}_test_best_discriminator.model\"))\n",
    "gen.eval()\n",
    "disc.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "print('Loading data')\n",
    "path = '/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/test/test_graphs_maps.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    nov_graphs,nov_maps = pickle.load(f)\n",
    "\n",
    "\n",
    "path = '/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/test/test_nodeids.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    nov_node_ids = pickle.load(f)\n",
    "\n",
    "from Dataset import *\n",
    "test_data = Dataset(nov_graphs,nov_maps, ids =  nov_node_ids)\n",
    "targets = test_data.make_targets_from_ids()\n",
    "test_data.y = targets\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False, collate_fn = my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,r,f1,acc,X,y,pred= models.validate_model(disc,test_loader, return_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.5344827586206896, 1.0, 0.6966292134831461, 0.9318181818181818)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "p,r,f1,acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}