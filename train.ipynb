{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bitdglconda2e96401c5587439a8bf114176530353d",
   "display_name": "Python 3.7.9 64-bit ('dgl': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(180000)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f34c5813af0>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import dgl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits as BCELoss\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "import torch.nn as nn\n",
    "\n",
    "import networkx as nx\n",
    "import json\n",
    "import models\n",
    "device = \"cuda\"\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading data\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8c3a46201657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/novelgridworlds_nonov_1_easy/normal_graphs_maps.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mgraphs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/storage.py\u001b[0m in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_load_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "print('Loading data')\n",
    "path = '/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/novelgridworlds_nonov_1_easy/normal_graphs_maps.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    graphs,maps = pickle.load(f)\n",
    "\n",
    "\n",
    "path = '/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/novelgridworlds_nonov_1_easy/normal_nodeids.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    node_ids = pickle.load(f)\n",
    "\n",
    "from Dataset import *\n",
    "\n",
    "data = Dataset(graphs,maps)\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn = my_collate)\n",
    "\n",
    "\n",
    "print('Loading data')\n",
    "#path = '/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/novelgridworlds_axe_1_hard/novel_graphs_maps.pkl'\n",
    "path = '/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/valid/valid_graphs_maps.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    nov_graphs,nov_maps = pickle.load(f)\n",
    "\n",
    "\n",
    "#path = '/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/novelgridworlds_axe_1_hard/novel_nodeids.pkl'\n",
    "path = '/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/valid/valid_nodeids.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    nov_node_ids = pickle.load(f)\n",
    "\n",
    "from Dataset import *\n",
    "test_data = Dataset(nov_graphs,nov_maps, ids =  nov_node_ids)\n",
    "targets = test_data.make_targets_from_ids()\n",
    "test_data.y = targets\n",
    "valid_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False, collate_fn = my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "\n",
    "def train_discriminator(disc, gen, batch_size,device,batch, opt_d):\n",
    "\n",
    "    pair = []\n",
    "    opt_d.zero_grad()\n",
    "    \n",
    "    graphs1,maps1,graphs2,maps2, real_targets = batch\n",
    "\n",
    "    graphs1 = graphs1.to(device)\n",
    "    graphs2 = graphs2.to(device)\n",
    "\n",
    "    maps1 = maps1.to(device)\n",
    "    maps2 = maps2.to(device)\n",
    "    real_targets = torch.Tensor(real_targets).to(device)\n",
    "\n",
    "    \n",
    "\n",
    "    steps = [graphs1,maps1,graphs2,maps2]\n",
    "    real_preds, im1 = disc(steps)\n",
    "    pair.append(im1.detach().cpu())\n",
    "\n",
    "    real_loss = torch.nn.functional.binary_cross_entropy(real_preds, real_targets)\n",
    "    real_score = torch.mean(real_preds).item()\n",
    "    \n",
    "    \n",
    "    action = gen(steps)\n",
    "    \n",
    "    fake_steps = models.make_step_modifications(action, steps)\n",
    "    \n",
    "    graphs1,maps1,graphs2,maps2, fake_targets = fake_steps\n",
    "    graphs1 = graphs1.to(device)\n",
    "    graphs2 = graphs2.to(device)\n",
    "    fake_targets = torch.zeros(len(maps2)).to(device)\n",
    "\n",
    "    maps1 = maps1.to(device)\n",
    "    maps2 = maps2.to(device)\n",
    "\n",
    "    fake_steps = [graphs1,maps1,graphs2,maps2]\n",
    "    \n",
    "    \n",
    "    \n",
    "    fake_preds,im2 = disc(fake_steps)\n",
    "    pair.append(im2.detach().cpu())\n",
    "    fake_loss = torch.nn.functional.binary_cross_entropy(fake_preds, fake_targets)\n",
    "    fake_score = torch.mean(fake_preds).item()\n",
    "\n",
    "    \n",
    "    imgs.append(pair)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    total_loss.backward()\n",
    "\n",
    "    opt_d.step()\n",
    "    return total_loss.item(), real_score, fake_score\n",
    "\n",
    "\n",
    "\n",
    "def train_generator(gen, disc, batch_size, opt_g, batch, device):\n",
    "    \n",
    "    opt_g.zero_grad()\n",
    "\n",
    "    graphs1,maps1,graphs2,maps2, real_targets = batch\n",
    "    graphs1 = graphs1.to(device)\n",
    "    graphs2 = graphs2.to(device)\n",
    "\n",
    "    maps1 = maps1.to(device)\n",
    "    maps2 = maps2.to(device)\n",
    "    real_targets = torch.ones(len(maps2)).to(device)\n",
    "\n",
    "    steps = [graphs1,maps1,graphs2,maps2]\n",
    "\n",
    "    action = gen(steps)\n",
    "    \n",
    "\n",
    "    fake_steps = make_step_modifications(action, steps)\n",
    "    \n",
    "\n",
    "    graphs1,maps1,graphs2,maps2, fake_targets = fake_steps\n",
    "    graphs1 = graphs1.to(device)\n",
    "    graphs2 = graphs2.to(device)\n",
    "\n",
    "    maps1 = maps1.to(device)\n",
    "    maps2 = maps2.to(device)\n",
    "    fake_targets = torch.zeros(len(maps2)).to(device)#torch.Tensor(fake_targets).to(device)\n",
    "\n",
    "    fake_steps = [graphs1,maps1,graphs2,maps2]\n",
    "    \n",
    "    \n",
    "    preds,_ = disc(fake_steps)\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss = torch.nn.functional.binary_cross_entropy(preds, fake_targets)\n",
    "    \n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    opt_g.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Training function. Takes in Discriminator and Generator. Generator can be just a random sampler of action. \n",
    "\n",
    "def fit(disc, gen, train_loader,epochs, device,lr_g = 1e-4, lr_d = 1e-4, start_idx=1, run_on_valid = False, test_loader = None,batch_size = 32, model_name = \"novelty_gan\"):\n",
    "    model_name = model_name+\"_best\"\n",
    "    f1s = []\n",
    "    torch.cuda.empty_cache()\n",
    "    disc.cuda()\n",
    "    gen.cuda()\n",
    "    disc.train()\n",
    "    gen.train()\n",
    "    # Losses & scores\n",
    "    losses_g = []\n",
    "    losses_d = []\n",
    "    real_scores = []\n",
    "    fake_scores = []\n",
    "    best_val_f1 = -np.inf\n",
    "    # Create optimizers\n",
    "    opt_d = torch.optim.Adam(disc.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "    opt_g = None\n",
    "    if gen is not None and type(gen).__name__ != \"RandomGenerator\":\n",
    "        opt_g = torch.optim.Adam(gen.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss_g = 0\n",
    "        loss_d = 0\n",
    "        counter = 0\n",
    "        real_score = 0\n",
    "        fake_score = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "            counter+=1\n",
    "            \n",
    "            # Train discriminator\n",
    "            loss_d, real_score, fake_score = train_discriminator(disc,gen,batch_size,device,batch, opt_d)\n",
    "            # Train generator\n",
    "            if opt_g is not None:\n",
    "                for j in range(1):#if counter % 1 == 0:#for j in range(5):\n",
    "                    \n",
    "                    loss_g = train_generator(gen,disc,batch_size,opt_g,batch, device)\n",
    "            else:\n",
    "                loss_g = 0\n",
    "                \n",
    "                \n",
    "        losses_g.append(loss_g)\n",
    "        losses_d.append(loss_d)\n",
    "        real_scores.append(real_score)\n",
    "        fake_scores.append(fake_score)\n",
    "        \n",
    "        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
    "            epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n",
    "        \n",
    "    \n",
    "        if run_on_valid:\n",
    "            precision, recall,f1,score = models.validate_model(disc,test_loader)\n",
    "            if f1>best_val_f1:\n",
    "                  best_val_f1 = f1\n",
    "                  torch.save(gen.state_dict(),os.path.join(\"./models\",model_name+\"_generator.model\"))\n",
    "                  torch.save(disc.state_dict(),os.path.join(\"./models\",model_name+\"_discriminator.model\"))\n",
    "                  file = open(os.path.join(\"./models\",model_name+\"_stats.json\"),'w')\n",
    "                  json.dump({\"precision\":precision,\"recall\":recall,\"f1\":f1},file)\n",
    "                  file.close()\n",
    "            f1s.append(f1)\n",
    "        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}, valid_precision: {:.4f}, valid_recall: {:.4f}, valid_F1: {:.4f}, valid_acc: {:.4f}\".format(\n",
    "            epoch+1, epochs, loss_g, loss_d, real_score, fake_score, precision,recall,f1, score))\n",
    "    \n",
    "    return losses_g, losses_d, real_scores, fake_scores, f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize models and Fit. \n",
    "\n",
    "disc = models.MapGraphModel(50,16)\n",
    "device = \"cuda\"\n",
    "disc.to(device)\n",
    "gen =models.RandomGenerator(batch_size)#MapGraphNoveltyInjector(50,16)\n",
    "gen.to(device)\n",
    "\n",
    "\n",
    "#opt_g = torch.optim.Adam(gen.parameters(), lr = 1e-3, betas=(0.5, 0.999))\n",
    "epochs = 10\n",
    "\n",
    "%time f1s_replay = fit(disc, gen, train_loader,epochs, device = device,lr_d = 1e-3, lr_g = 1e-3, test_loader=valid_loader, run_on_valid= True,batch_size = batch_size, model_name=f\"novelty_gan_{model_predictor}_test\")[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = models.MapGraphModel(50,16)\n",
    "device = \"cuda\"\n",
    "disc.to(device)\n",
    "gen = models.RandomGenerator(batch_size)#MapGraphNoveltyInjector(50,8)\n",
    "gen.to(device)\n",
    "gen.load_state_dict(torch.load(f\"./models/novelty_gan_{model_predictor}_test_best_generator.model\"))\n",
    "disc.load_state_dict(torch.load(f\"./models/novelty_gan_{model_predictor}_test_best_discriminator.model\"))\n",
    "gen.eval()\n",
    "disc.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading data')\n",
    "path = '/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/test/test_graphs_maps.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    nov_graphs,nov_maps = pickle.load(f)\n",
    "\n",
    "\n",
    "path = '/media/panagiotis/TOSHIBA EXT1/Research/Novelty_detection/datasets/gridworlds_data/test/test_nodeids.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    nov_node_ids = pickle.load(f)\n",
    "\n",
    "from Dataset import *\n",
    "test_data = Dataset(nov_graphs,nov_maps, ids =  nov_node_ids)\n",
    "targets = test_data.make_targets_from_ids()\n",
    "test_data.y = targets\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False, collate_fn = my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,r,f1,acc,X,y,pred= models.validate_model(disc,test_loader, return_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,r,f1,acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}